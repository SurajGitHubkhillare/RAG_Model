# -*- coding: utf-8 -*-
"""RAG_P&L_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1920BF9tfrn_FXP6rTH8o3lSU6SZKh2CT

# Importing Required Libraries
"""

!pip install langchain-pinecone langchain langchain-community langchain-google-genai pypdf streamlit pyngrok

%%writefile app.py
import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.prompts import PromptTemplate
import pinecone
from langchain_pinecone import PineconeVectorStore
import os

pinecone_key = < use your pinecone api_key >  #####################use your pinecone api_key
os.environ['PINECONE_API_KEY'] = pinecone_key
google_api_key = < ai studio api_key >     ######################### use your ai studio api_key

st.title("Financial AI Assistant - RAG Model")
st.subheader("Upload a Financial Statement PDF and Ask Questions")

uploaded_file = st.file_uploader("Upload PDF", type=["pdf"])

if uploaded_file is not None:
    pdf_path = f"temp_{uploaded_file.name}"
    with open(pdf_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=768, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)

    embeddings = GoogleGenerativeAIEmbeddings(
        model='models/embedding-001',
        google_api_key=google_api_key,
        task_type="retrieval_query"
    )

    index_name = "rag-index1"
    docsearch = PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)

    prompt_template = """
    You are a financial AI assistant specializing in analyzing Profit & Loss (P&L) statements.
    Your task is to answer user queries based on the provided financial data.

    ### Context:
    {context}

    ### User Question:
    {question}

    ### Guidelines:
    1. **Extract only relevant information** from the context.
    2. **Provide structured responses** (e.g., bullet points, tables, or calculations).
    3. **Include necessary calculations** and financial insights where applicable.
    4. **Avoid assumptions**â€”state explicitly if data is missing.
    5. **Ensure clarity and conciseness** while maintaining informative depth.

    ### Answer:
    """

    prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'query'])

    chat_model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=google_api_key, temperature=0.3)
    retriever_from_llm = MultiQueryRetriever.from_llm(retriever=docsearch.as_retriever(search_kwargs={"k": 5}),
                                                      llm=chat_model)
    qa_chain = RetrievalQA.from_chain_type(llm=chat_model, retriever=retriever_from_llm, return_source_documents=True,
                                           chain_type="stuff", chain_type_kwargs={"prompt": prompt})

    user_question = st.text_input("Enter your financial question:")
    if st.button("Get Response"):
        if user_question:
            # response = qa_chain.invoke({"question": user_question})
            response = qa_chain.invoke({"query": user_question})

            st.subheader("Response:")
            st.write(response["result"])
        else:
            st.warning("Please enter a question.")


from pyngrok import ngrok
ngrok.kill()
outh_token = < use your ngrok authentication token >  ############### use your ngrok authentication token 
ngrok.set_auth_token(outh_token)

#create the tunnel
ngrok_tunnel=ngrok.connect(addr="5000",proto="http")
print("Tricking url :",ngrok_tunnel.public_url)

!streamlit run --server.port 5000 app.py
#!streamlit run --server.port 5000 app.py>/dev/null
