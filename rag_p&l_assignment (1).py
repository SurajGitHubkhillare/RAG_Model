# -*- coding: utf-8 -*-
"""RAG_P&L_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1920BF9tfrn_FXP6rTH8o3lSU6SZKh2CT

# Importing Required Libraries
"""

!pip install langchain-pinecone langchain langchain-community langchain-google-genai pypdf

!pip install langchain-pinecone langchain langchain-community langchain-google-genai pymupdf
from langchain.prompts import PromptTemplate # Prompt template
from langchain.vectorstores import Chroma   # Store the vectors
from langchain.text_splitter import RecursiveCharacterTextSplitter # Chunks
from langchain.chains import VectorDBQA,RetrievalQA, LLMChain # Chains and Retrival ans
from langchain.retrievers.multi_query import MultiQueryRetriever # Multiple Answers
from langchain_google_genai import ChatGoogleGenerativeAI # GenAI model to retrive
from langchain_google_genai import GoogleGenerativeAIEmbeddings # GenAI model to conver words
## load the data
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("/content/Sample Financial Statement.pdf")  # Load the document, split it into chunks, and embed each chunk.
documents = loader.load()
## divide into chunk
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
#set upp model
# Set up embeddings
embeddings = GoogleGenerativeAIEmbeddings(
    model='models/embedding-001',
    google_api_key='AIzaSyAUH70gKFSmR52QAbZq4fJFM3WSbTYCHp8',
    task_type="retrieval_query"
)
from google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory

safety_settings = {
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                    }

chat_model = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    google_api_key='AIzaSyAUH70gKFSmR52QAbZq4fJFM3WSbTYCHp8',
    temperature=0.3,
    safety_settings=safety_settings
)
# get the embedding store in vector
from langchain_pinecone import PineconeVectorStore
import os


pinecone_key="pcsk_69p97j_JqideQuYSZWE23jaZok8zMHPRhxLKdcADeNr6Gwb27hhkTFVifcgfo9j2f3hvVg"
os.environ['PINECONE_API_KEY'] = pinecone_key

index_name = "rag-index1"

# Connect to Pinecone index and insert the chunked docs as contents
docsearch = PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)

# prompt templet
prompt_template = """
You are a financial AI assistant specializing in analyzing Profit & Loss (P&L) statements. Your task is to answer user queries based on the provided financial data.

### Context:
{context}

### User Question:
{question}

### Guidelines:
1. **Extract only relevant information** from the context.
2. **Provide structured responses** (e.g., bullet points, tables, or calculations).
3. **Include necessary calculations** and financial insights where applicable.
4. **Avoid assumptions**—state explicitly if data is missing.
5. **Ensure clarity and conciseness** while maintaining informative depth.

### Answer:
"""

# Define the prompt template
prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])
# Create the QA
retriever_from_llm = MultiQueryRetriever.from_llm(retriever=docsearch.as_retriever(search_kwargs={"k": 5}),
                                                  llm=chat_model)

qa_chain = RetrievalQA.from_chain_type(llm=chat_model,
                                       retriever= retriever_from_llm,
                                       return_source_documents=True,
                                       chain_type="stuff",
                                       chain_type_kwargs={"prompt": prompt}
                                      )
##response

response = qa_chain.invoke({"What is the gross profit for Q3 2024?"})
print(response["result"])

response2 = qa_chain.invoke({"How do the net income and operating expenses compare for Q1 2024?"})
print(response2["result"])

response3 = qa_chain.invoke({"How do the net income and operating expenses compare for Q4 2024?"})
print(response3["result"])

"""# Load the data"""

from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("/content/Sample Financial Statement.pdf")  # Load the document, split it into chunks, and embed each chunk.
documents = loader.load()



"""# divide into chunks"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

"""# Set up the Model
- one is embedding model
- one is chat model
"""

# Set up embeddings
embeddings = GoogleGenerativeAIEmbeddings(
    model='models/embedding-001',
    google_api_key='AIzaSyAUH70gKFSmR52QAbZq4fJFM3WSbTYCHp8',
    task_type="retrieval_query"
)

from google.generativeai.types.safety_types import HarmBlockThreshold, HarmCategory

safety_settings = {
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                    }

chat_model = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    google_api_key='AIzaSyAUH70gKFSmR52QAbZq4fJFM3WSbTYCHp8',
    temperature=0.3,
    safety_settings=safety_settings
)

"""# Get The Embedding Store In Vector



"""

from langchain_pinecone import PineconeVectorStore
import os


pinecone_key="pcsk_69p97j_JqideQuYSZWE23jaZok8zMHPRhxLKdcADeNr6Gwb27hhkTFVifcgfo9j2f3hvVg"
os.environ['PINECONE_API_KEY'] = pinecone_key

index_name = "rag-index1"

# Connect to Pinecone index and insert the chunked docs as contents
docsearch = PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)

prompt_template = """
You are a financial AI assistant specializing in analyzing Profit & Loss (P&L) statements. Your task is to answer user queries based on the provided financial data.

### Context:
{context}

### User Question:
{question}

### Guidelines:
1. **Extract only relevant information** from the context.
2. **Provide structured responses** (e.g., bullet points, tables, or calculations).
3. **Include necessary calculations** and financial insights where applicable.
4. **Avoid assumptions**—state explicitly if data is missing.
5. **Ensure clarity and conciseness** while maintaining informative depth.

### Answer:
"""

# Define the prompt template
prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])

# Create the QA
retriever_from_llm = MultiQueryRetriever.from_llm(retriever=docsearch.as_retriever(search_kwargs={"k": 5}),
                                                  llm=chat_model)

qa_chain = RetrievalQA.from_chain_type(llm=chat_model,
                                       retriever= retriever_from_llm,
                                       return_source_documents=True,
                                       chain_type="stuff",
                                       chain_type_kwargs={"prompt": prompt}
                                      )

response = qa_chain.invoke({"What is the gross profit for Q3 2024?"})
print(response["result"])

response2 = qa_chain.invoke({"How do the net income and operating expenses compare for Q1 2024?"})
print(response2["result"])

response3 = qa_chain.invoke({"How do the net income and operating expenses compare for Q4 2024?"})
print(response3["result"])

!pip install langchain-pinecone langchain langchain-community langchain-google-genai pypdf





!pip install streamlit
!npm install localtunnel

!pip install pyngrok

import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.prompts import PromptTemplate
import pinecone
from langchain_pinecone import PineconeVectorStore
from pyngrok import ngrok
import os


pinecone_key="pcsk_69p97j_JqideQuYSZWE23jaZok8zMHPRhxLKdcADeNr6Gwb27hhkTFVifcgfo9j2f3hvVg"
os.environ['PINECONE_API_KEY'] = pinecone_key
google_api_key = "AIzaSyA0HD4gzsSjgAnfH2AseyVsFANQ6427Ado"

#index_name = "rag-index1"

# Initialize Streamlit App
st.title("Financial AI Assistant - RAG Model")
st.subheader("Upload a Financial Statement PDF and Ask Questions")

# File uploader
uploaded_file = st.file_uploader("Upload PDF", type=["pdf"])

if uploaded_file is not None:
    # Save uploaded file
    pdf_path = f"temp_{uploaded_file.name}"
    with open(pdf_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # Load and process PDF
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)

    # Set up embeddings
    embeddings = GoogleGenerativeAIEmbeddings(
        model='models/embedding-001',
        google_api_key=google_api_key,
        task_type="retrieval_query"
    )

    # Set up vector store
    index_name = "rag-index1"
    docsearch = PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)

    # Define prompt template
    prompt_template = """
    You are a financial AI assistant specializing in analyzing Profit & Loss (P&L) statements.
    Your task is to answer user queries based on the provided financial data.

    ### Context:
    {context}

    ### User Question:
    {question}

    ### Guidelines:
    1. **Extract only relevant information** from the context.
    2. **Provide structured responses** (e.g., bullet points, tables, or calculations).
    3. **Include necessary calculations** and financial insights where applicable.
    4. **Avoid assumptions**—state explicitly if data is missing.
    5. **Ensure clarity and conciseness** while maintaining informative depth.

    ### Answer:
    """

    # prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])
    prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'query'])

    # Initialize LLM and retriever
    chat_model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=google_api_key, temperature=0.3)
    retriever_from_llm = MultiQueryRetriever.from_llm(retriever=docsearch.as_retriever(search_kwargs={"k": 5}),
                                                      llm=chat_model)
    qa_chain = RetrievalQA.from_chain_type(llm=chat_model, retriever=retriever_from_llm, return_source_documents=True,
                                           chain_type="stuff", chain_type_kwargs={"prompt": prompt})

    # User query input
    user_question = st.text_input("Enter your financial question:")
    if st.button("Get Response"):
        if user_question:
            response = qa_chain.invoke({"question": user_question})
            st.subheader("Response:")
            st.write(response["result"])
        else:
            st.warning("Please enter a question.")

# Set up Ngrok for public access

pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from langchain.document_loaders import PyPDFLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
# from langchain.chains import RetrievalQA
# from langchain.retrievers.multi_query import MultiQueryRetriever
# from langchain.prompts import PromptTemplate
# import pinecone
# from langchain_pinecone import PineconeVectorStore
# import os
# 
# pinecone_key = "pcsk_69p97j_JqideQuYSZWE23jaZok8zMHPRhxLKdcADeNr6Gwb27hhkTFVifcgfo9j2f3hvVg"
# os.environ['PINECONE_API_KEY'] = pinecone_key
# google_api_key = "AIzaSyA0HD4gzsSjgAnfH2AseyVsFANQ6427Ado"
# 
# st.title("Financial AI Assistant - RAG Model")
# st.subheader("Upload a Financial Statement PDF and Ask Questions")
# 
# uploaded_file = st.file_uploader("Upload PDF", type=["pdf"])
# 
# if uploaded_file is not None:
#     pdf_path = f"temp_{uploaded_file.name}"
#     with open(pdf_path, "wb") as f:
#         f.write(uploaded_file.getbuffer())
# 
#     loader = PyPDFLoader(pdf_path)
#     documents = loader.load()
#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=768, chunk_overlap=0)
#     texts = text_splitter.split_documents(documents)
# 
#     embeddings = GoogleGenerativeAIEmbeddings(
#         model='models/embedding-001',
#         google_api_key=google_api_key,
#         task_type="retrieval_query"
#     )
# 
#     index_name = "rag-index1"
#     docsearch = PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)
# 
#     prompt_template = """
#     You are a financial AI assistant specializing in analyzing Profit & Loss (P&L) statements.
#     Your task is to answer user queries based on the provided financial data.
# 
#     ### Context:
#     {context}
# 
#     ### User Question:
#     {question}
# 
#     ### Guidelines:
#     1. **Extract only relevant information** from the context.
#     2. **Provide structured responses** (e.g., bullet points, tables, or calculations).
#     3. **Include necessary calculations** and financial insights where applicable.
#     4. **Avoid assumptions**—state explicitly if data is missing.
#     5. **Ensure clarity and conciseness** while maintaining informative depth.
# 
#     ### Answer:
#     """
# 
#     prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'query'])
# 
#     chat_model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=google_api_key, temperature=0.3)
#     retriever_from_llm = MultiQueryRetriever.from_llm(retriever=docsearch.as_retriever(search_kwargs={"k": 5}),
#                                                       llm=chat_model)
#     qa_chain = RetrievalQA.from_chain_type(llm=chat_model, retriever=retriever_from_llm, return_source_documents=True,
#                                            chain_type="stuff", chain_type_kwargs={"prompt": prompt})
# 
#     user_question = st.text_input("Enter your financial question:")
#     if st.button("Get Response"):
#         if user_question:
#             # response = qa_chain.invoke({"question": user_question})
#             response = qa_chain.invoke({"query": user_question})
# 
#             st.subheader("Response:")
#             st.write(response["result"])
#         else:
#             st.warning("Please enter a question.")
#

from pyngrok import ngrok
ngrok.kill()
outh_token="2sQcLG8VK3dhoYOHLFg9AUaRAYC_6KUQ5xf3999QAyoCiLign"
ngrok.set_auth_token(outh_token)

#create the tunnel
ngrok_tunnel=ngrok.connect(addr="5000",proto="http")
print("Tricking url :",ngrok_tunnel.public_url)

!streamlit run --server.port 5000 app.py
#!streamlit run --server.port 5000 app.py>/dev/null

